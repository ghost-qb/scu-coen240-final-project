"""
Santa Clara University

COEN 240 - Machine Learning

Final Project

Quan Bach
Anh Truong


This file is LDA for only the top 20,000 words in vocabulary

By running thi file: 
    - a table of n topics will be displayed in output (n = 8 in this script and it can be changed)
    - two html files will be saved to a folder named LDA under the name lda-bow.html and lda-tfidf.html
    - two html files is an interactive visualization of the lda models trained by bags of words and tf-idf
    
    
*** NOTICE ***: this script will dowload the 20 news group dataset from sklearn and perform processing directly here before the lda models. 
Therefore, it does not used the vocabularies generated by other files previously. However, the content should be the same. 


"""
import os
import gensim
import pyLDAvis.gensim
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from sklearn.datasets import fetch_20newsgroups
from gensim import models
import numpy as np



# Tokenize and lemmatize

def lemmatize_stemming(text):
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))
def preprocess(text):
    result=[]
    for token in gensim.utils.simple_preprocess(text) :
        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:
            result.append(lemmatize_stemming(token))
            
    return result


if __name__ == '__main__':
    
    if not os.path.exists('./LDA/'):
           os.makedirs('./LDA/')
    
    stemmer = SnowballStemmer("english")
    # load data 
    newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle = True)    
    
    # initialize empty list to hold all docs 
    processed_docs = []
    
     # process the docs
    print('Loading dataset.....')
    for idx in range(0,len(newsgroups_train.data)):
        print(str(newsgroups_train.filenames[idx]))
        processed_docs.append(preprocess(newsgroups_train.data[idx]))
        
    print()
    print('Dataset loaded....')
    print('...')
    
            
    dictionary = gensim.corpora.Dictionary(processed_docs)


    

    
    # Remove very rare and very common words:
    # -words appearing less than 15 times
    # -words appearing in more than 10% of all documents
    # -keep only the first 20,000 most frequent tokens
    dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n= 20000)
    
    
    print ('LDA with 20k size vocab training with bags of words in progress...')
    print('...')
    # Create a bag or word for each document 
    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]


    # train LDA model with bags of words 
    lda_model_bow =  gensim.models.LdaMulticore(bow_corpus, 
                                   num_topics = 12, 
                                   id2word = dictionary,                                    
                                   passes = 10,
                                   workers = 2,
                                   minimum_probability=0.0)
    
    np.save('./npy/lda_bow_topic_distribution_20k.npy',lda_model_bow[bow_corpus])
    print('Saved LDA bags of words topic distribution to npy folder...')
    print('...')
    
    print('**** LDA MODEL USING BAGS OF WORDS with 20k vocab *****')
    # Print out the words in each topic and its weight 
    for idx, topic in lda_model_bow.print_topics(-1):
        print("Topic: {} \nWords: {}".format(idx, topic ))
        print()
        
    print('*' * 20)
    print()
    
    lda_bow_vis = pyLDAvis.gensim.prepare(lda_model_bow, bow_corpus, dictionary=lda_model_bow.id2word)  
        
    # save visualization to html
    pyLDAvis.save_html(lda_bow_vis, './LDA/lda-bow-20k.html')
    print('...')
    print('Saved lda-bagofwords visualization to LDA folder')
    print('...')
    
    # performing tf-idf from bags of words
    
    tfidf = models.TfidfModel(bow_corpus)
    tfidf_corpus = tfidf[bow_corpus]
    
    
    print ('LDA with 20k size vocab training with TF-IDF in progress...')
    print('...')
    lda_model_tfidf =  gensim.models.LdaMulticore(tfidf_corpus, 
                                   num_topics = 12, 
                                   id2word = dictionary,                                    
                                   passes = 10,
                                   workers = 2,
                                   minimum_probability=0.0)
    
    np.save('./npy/lda_tfidf_topic_distribution_20k.npy',lda_model_tfidf[tfidf_corpus])
    print('Saved LDA TF-IDF topic distribution to npy folder...')
    print('...')
    
    print('**** LDA MODEL USING TF-IDF with 20k vocab  *****')
    # Print out the words in each topic and its weight 
    for idx, topic in lda_model_tfidf.print_topics(-1):
        print("Topic: {} \nWords: {}".format(idx, topic ))
        print()
        
    print('*' * 20)
    print()
    
    lda_tfidf_vis = pyLDAvis.gensim.prepare(lda_model_tfidf, tfidf_corpus, dictionary=lda_model_tfidf.id2word)  
        
    # save visualization to html
    pyLDAvis.save_html(lda_tfidf_vis, './LDA/lda-tfidf-20k.html')
    print('...')
    print('Saved lda-tfidf visualization to LDA folder')